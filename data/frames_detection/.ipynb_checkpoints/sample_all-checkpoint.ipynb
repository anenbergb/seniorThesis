{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "experiment_dir = '/afs/cs.stanford.edu/u/anenberg/scr/snrThesis/data/frames_detection/lists'\n",
    "filenames = ['fullpath_train_list.txt', 'fullpath_test_list.txt']\n",
    "num_samples = 5\n",
    "optical_flow_offset = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 25 22 21 23]\n"
     ]
    }
   ],
   "source": [
    "def cut_and_randomly_select(peaks, num_samples = 5, optical_flow_offset = 30,offset = 0):\n",
    "    \"\"\"\n",
    "        peaks - an array [10, 19, 30, 80]\n",
    "        optical_flow_offset - make sure that the frame and the frame + optical_flow_offset are in 'good' regions\n",
    "        offset - a number to add to all the indices\n",
    "\n",
    "if optical_flow_offset = 0, then don't worry if the corresponding optical flow frame is in bounds.\n",
    "    \"\"\"\n",
    "    peaks_full = peaks\n",
    "    # zip the peaks together to get the bounds e.g. [10, 19, 30, 80] --> [(10, 19), (19, 30), (30, 80)]\n",
    "    df = pd.DataFrame(zip(peaks_full[:-1], peaks_full[1:]), columns = ['left', 'right'])\n",
    "\n",
    "    df['scene_length'] = df.right - df.left\n",
    "    \n",
    "    # group is valid only if the bound length is greater than the median bound length\n",
    "    #df['valid'] = df.scene_length >= df.scene_length.median()\n",
    "    \n",
    "    #group is valid only if the bound length is within the middle 3/5 of the bound length distribution.\n",
    "    df['valid'] = np.logical_and(df.scene_length >= 0.2*df.scene_length.max(),df.scene_length < 0.8*df.scene_length.max())\n",
    "    valid_df = df[df.valid == True]\n",
    "    \n",
    "    valid_indices = []\n",
    "    for l, r in zip(valid_df.left.values, valid_df.right.values):\n",
    "        valid_indices.extend(range(l, r))\n",
    "    \n",
    "    # making sure the optical flow is valid\n",
    "    valid_indices = [i for i in valid_indices if i + optical_flow_offset in valid_indices]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        # select num_samples at random\n",
    "        return np.random.choice(valid_indices, min(num_samples, len(valid_indices)), replace=False) + offset\n",
    "    \n",
    "print cut_and_randomly_select([10, 18, 30, 80, 90], num_samples=5, optical_flow_offset=0, offset = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86, 87], [80, 81], [24, 25], [26, 27], [18, 19]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def KLsamples(distribution,k,l):\n",
    "    \"\"\"\n",
    "    k: number of sample/sets to draw\n",
    "    l: length of consecutive integers in a sample/set\n",
    "    \n",
    "    returns a length k list of lists of length l. \n",
    "    \"\"\"\n",
    "    num_bins = len(distribution)/l\n",
    "    bins = range(num_bins)\n",
    "    k_to_sample = min(num_bins,k)\n",
    "    sampled_bins = np.random.choice(bins,k_to_sample, replace=False)\n",
    "    \n",
    "    returnLists = []\n",
    "    for b in sampled_bins:\n",
    "        segment = distribution[b*l:(b+1)*l]\n",
    "        assert len(segment) == l\n",
    "        returnLists.append(segment)\n",
    "    return returnLists\n",
    "\n",
    "def cut_and_randomly_select_stacked(peaks, num_samples = 5, stacked=1, offset = 0):\n",
    "    \"\"\"\n",
    "        peaks - an array [10, 19, 30, 80]\n",
    "        offset - a number to add to all the indices\n",
    "        stacked - number of consecutive frames to include in one sample set (1 = just the single frame)\n",
    "        if optical_flow_offset = 0, then don't worry if the corresponding optical flow frame is in bounds.\n",
    "    \"\"\"\n",
    "    peaks_full = peaks\n",
    "    # zip the peaks together to get the bounds e.g. [10, 19, 30, 80] --> [(10, 19), (19, 30), (30, 80)]\n",
    "    df = pd.DataFrame(zip(peaks_full[:-1], peaks_full[1:]), columns = ['left', 'right'])\n",
    "\n",
    "    df['scene_length'] = df.right - df.left\n",
    "    \n",
    "    # group is valid only if the bound length is greater than the median bound length\n",
    "    #df['valid'] = df.scene_length >= df.scene_length.median()\n",
    "    \n",
    "    #group is valid only if the bound length is within the middle 3/5 of the bound length distribution.\n",
    "    df['valid'] = np.logical_and(df.scene_length >= 0.2*df.scene_length.max(),df.scene_length < 0.8*df.scene_length.max())\n",
    "    valid_df = df[df.valid == True]\n",
    "    \n",
    "    valid_indices = []\n",
    "    for l, r in zip(valid_df.left.values, valid_df.right.values):\n",
    "        valid_indices.extend(range(l, r))\n",
    "    \n",
    "    # making sure the optical flow is valid\n",
    "    valid_indices = [i for i in valid_indices if i + stacked-1 in valid_indices]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        # select num_samples at random\n",
    "        return KLsamples(valid_indices, num_samples,stacked)\n",
    "        #flat = [x for sublist in indices_lists for x in sublist] \n",
    "        #return flat\n",
    "\n",
    "print cut_and_randomly_select_stacked([10, 18, 31, 80, 90], num_samples=5, stacked=2, offset = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks = [10, 19, 30, 80]\n",
    "peaks_full = peaks\n",
    "df = pd.DataFrame(zip(peaks_full[:-1], peaks_full[1:]), columns = ['left', 'right'])\n",
    "df['scene_length'] = df.right - df.left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(experiment_dir, filenames[0]), delimiter = ' ', header = None, names = ['filename', 'class_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8691191"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index[:-optical_flow_offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing shuffle_sampled_t1_5_fullpath_train_list.txt\n",
      "Loaded fullpath_train_list.txt\n",
      "Computed Boundaries\n",
      "Finding video samples\n",
      "Done video samples\n",
      "0\n",
      "writing shuffle_sampled_t1_5_full_path_test_list.txt\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File /afs/cs.stanford.edu/u/anenberg/scr/snrThesis/data/frames_detection/lists/full_path_test_list.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-91bb2d9bca2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s_%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shuffle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'writing %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'Loaded %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3143)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5765)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File /afs/cs.stanford.edu/u/anenberg/scr/snrThesis/data/frames_detection/lists/full_path_test_list.txt does not exist"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "is_shuffle = True\n",
    "is_use_shot_detection = False\n",
    "output_file_number = 1\n",
    "\n",
    "for filename in filenames:\n",
    "    output_file = 'sampled_t%d_%d_%s'%(output_file_number,num_samples, filename)\n",
    "    if is_use_shot_detection:\n",
    "        output_file = 'shotdetect_sampled_t%d_%d_%s'%(output_file_number,num_samples, filename)\n",
    "    if is_shuffle:\n",
    "        output_file = '%s_%s'%('shuffle', output_file)\n",
    "    print 'writing %s'%output_file\n",
    "    df = pd.read_csv(os.path.join(experiment_dir, filename), delimiter = ' ', header = None, names = ['filename', 'class_id'])\n",
    "    print 'Loaded %s'%filename\n",
    "    \n",
    "    df['video_name'] = df.filename.apply(lambda x: x.split('/')[-2])\n",
    "    df['frame_id'] = df.filename.apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "    # find the locations of changes\n",
    "    flags = df.video_name != df.video_name.shift(1)\n",
    "    flags.append(pd.Series(1, [len(df)]))\n",
    "    breaks = pd.Series(np.where(flags == True)[0], dtype=np.uint32)\n",
    "    boundaries = zip(breaks, breaks.shift(-1))[:-1]\n",
    "    print 'Computed Boundaries'\n",
    "    \n",
    "    sub_indices = []\n",
    "\n",
    "    print 'Finding video samples'\n",
    "    num_failures = 0\n",
    "    for boundary in boundaries:\n",
    "        start, end = int(boundary[0]), int(boundary[1])\n",
    "        small_df = df[start:end]\n",
    "        if (end - start < num_samples):\n",
    "            sub_indices.append(range(start, end))\n",
    "        else:\n",
    "            video_name = df.video_name[start]\n",
    "\n",
    "            if is_use_shot_detection and (video_name.startswith('video_test') or video_name.startswith('video_valid')):\n",
    "                try:\n",
    "                    video_length_in_frames = end - start\n",
    "                    with open(os.path.join('/afs/cs.stanford.edu/u/anenberg/scr/CS231N/allFrames/sceneTransitions/', '%s.pkl'%df.video_name[start]), 'r+') as f:\n",
    "                        peaks = pickle.load(f)\n",
    "                    sub_indices.append(cut_and_randomly_select(peaks, num_samples=num_samples, optical_flow_offset =optical_flow_offset, offset = start))\n",
    "                except:\n",
    "                    sub_indices.append(np.random.choice(small_df.index, num_samples, replace=False))\n",
    "                    num_failures += 1\n",
    "            else:\n",
    "                if len(small_df.index) < optical_flow_offset+num_samples:\n",
    "                    continue\n",
    "                #don't sample from any frame whose optical flow pair is guarenteed to be out of range\n",
    "                if optical_flow_offset>0:\n",
    "                    sample_range = small_df.index[:-optical_flow_offset]\n",
    "                else:\n",
    "                    sample_range = small_df.index\n",
    "                sub_indices.append(np.random.choice(sample_range, num_samples, replace=False))\n",
    "    print 'Done video samples'\n",
    "    \n",
    "    \n",
    "    # turn a list of lists into a flat list\n",
    "    flat = [x for sublist in sub_indices for x in sublist] \n",
    "    # generate the sampled df from the indices\n",
    "    sampled_df = df.ix[flat]\n",
    "    if is_shuffle:\n",
    "        sampled_df = sampled_df.loc[np.random.permutation(sampled_df.index)]\n",
    "    # write the fields to csv\n",
    "    sampled_df[['filename', 'class_id']].to_csv(os.path.join(experiment_dir, output_file), header = False, index = False, sep = ' ')\n",
    "    print num_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing shuffle_stacked_5_shotdetect_sampled_t1_5_train_list.txt\n",
      "Loaded train_list.txt\n",
      "Computed Boundaries\n",
      "Finding video samples\n",
      "Done video samples\n",
      "272\n",
      "writing shuffle_stacked_5_shotdetect_sampled_t1_5_test_list.txt\n",
      "Loaded test_list.txt\n",
      "Computed Boundaries\n",
      "Finding video samples\n",
      "Done video samples\n",
      "481\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "is_shuffle = True\n",
    "is_use_shot_detection = True\n",
    "output_file_number = 1\n",
    "stacked = 5\n",
    "\n",
    "for filename in filenames:\n",
    "    output_file = 'stacked_%d_sampled_t%d_%d_%s'%(stacked,output_file_number,num_samples, filename)\n",
    "    if is_use_shot_detection:\n",
    "        output_file = 'stacked_%d_shotdetect_sampled_t%d_%d_%s'%(stacked,output_file_number,num_samples, filename)\n",
    "    if is_shuffle:\n",
    "        output_file = '%s_%s'%('shuffle', output_file)\n",
    "    print 'writing %s'%output_file\n",
    "    df = pd.read_csv(os.path.join(experiment_dir, filename), delimiter = ' ', header = None, names = ['filename', 'class_id'])\n",
    "    print 'Loaded %s'%filename\n",
    "    \n",
    "    df['video_name'] = df.filename.apply(lambda x: x.split('/')[0])\n",
    "    df['frame_id'] = df.filename.apply(lambda x: x.split('/')[1])\n",
    "\n",
    "    # find the locations of changes\n",
    "    flags = df.video_name != df.video_name.shift(1)\n",
    "    flags.append(pd.Series(1, [len(df)]))\n",
    "    breaks = pd.Series(np.where(flags == True)[0], dtype=np.uint32)\n",
    "    boundaries = zip(breaks, breaks.shift(-1))[:-1]\n",
    "    print 'Computed Boundaries'\n",
    "    \n",
    "    sub_indices = []\n",
    "\n",
    "    print 'Finding video samples'\n",
    "    num_failures = 0\n",
    "    for boundary in boundaries:\n",
    "        start, end = int(boundary[0]), int(boundary[1])\n",
    "        small_df = df[start:end]\n",
    "        tmp_num_samples = num_samples\n",
    "        #decrease number of samples until there are enough frames in video to sample .\n",
    "        while (end - start < num_samples*stacked):\n",
    "            tmp_num_samples -= 1\n",
    "        if tmp_num_samples >=0:\n",
    "            video_name = df.video_name[start]\n",
    "\n",
    "            if is_use_shot_detection and (video_name.startswith('video_test') or video_name.startswith('video_valid')):\n",
    "                try:\n",
    "                    video_length_in_frames = end - start\n",
    "                    with open(os.path.join('./data/allFrames/sceneTransitions/', '%s.pkl'%df.video_name[start]), 'r+') as f:\n",
    "                        peaks = pickle.load(f)\n",
    "                    #appends a list of lists\n",
    "                    sub_indices.append(cut_and_randomly_select_stacked(peaks, num_samples = tmp_num_samples, stacked=stacked, offset = start))\n",
    "                except: \n",
    "                    #pass, Don't want to randomly add indices.\n",
    "                    #sub_indices.append(np.random.choice(small_df.index, num_samples, replace=False))\n",
    "                    num_failures += 1\n",
    "            else:\n",
    "                #don't sample from any frame whose optical flow pair is guarenteed to be out of range\n",
    "                sub_indices.append(KLsamples(small_df.index,tmp_num_samples,stacked))\n",
    "    print 'Done video samples'\n",
    "    \n",
    "    #turn list of lists of lists int list of lists\n",
    "    flat = [x for sublist in sub_indices for x in sublist]\n",
    "    if is_shuffle: \n",
    "        flat = np.random.permutation(flat)\n",
    "    # turn a list of lists into a flat list\n",
    "    flat = [x for sublist in flat for x in sublist]\n",
    "    \n",
    "    # generate the sampled df from the indices\n",
    "    sampled_df = df.ix[flat]\n",
    "    # write the fields to csv\n",
    "    sampled_df[['filename', 'class_id']].to_csv(os.path.join(experiment_dir, output_file), header = False, index = False, sep = ' ')\n",
    "    print num_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = sampled_df[:10].sort('class_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8, 9], [10, 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 6], [1, 2, 3, 4], [10, 11], [7, 8, 9]], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1,2,3,4],[5,6]]\n",
    "B = [[7,8,9],[10,11]]\n",
    "C = [A, B]\n",
    "flat = [x for sublist in C for x in sublist]\n",
    "print flat\n",
    "np.random.permutation(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_ApplyEyeMakeup_g08_c01/00000089.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c01/00000105.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c01/00000073.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c01/00000024.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c01/00000083.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c02/00000083.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c02/00000049.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c02/00000073.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c02/00000101.jpg 0\r\n",
      "v_ApplyEyeMakeup_g08_c02/00000094.jpg 0\r\n"
     ]
    }
   ],
   "source": [
    "!head sampled_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  71530  143060 2709965 ../CS231N/data/allFrames/lists/shuffle_shotdetect_sampled_5_train_list.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc ../CS231N/data/allFrames/lists/shuffle_shotdetect_sampled_5_train_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "X = [1,2,3]\n",
    "random.sample(X,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 3), (4, 5), (5, 6), (7, 8), (8, 9)]\n"
     ]
    }
   ],
   "source": [
    "#df['valid'] = np.logical_and(df.scene_length >= 0.2*df.scene_length.max(),df.scene_length < 0.8*df.scene_length.max())\n",
    "\n",
    "A = [1,2,3,4,5,6,7,8,9]\n",
    "pairs = []\n",
    "for i in xrange(0,len(A),3):\n",
    "    for j in xrange(1,3):\n",
    "        pairs.append((A[i+j-1],A[i+j]))\n",
    "print pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032    video_test_0000001/00001033.jpg\n",
       "1033    video_test_0000001/00001034.jpg\n",
       "1034    video_test_0000001/00001035.jpg\n",
       "1035    video_test_0000001/00001036.jpg\n",
       "1036    video_test_0000001/00001037.jpg\n",
       "2393    video_test_0000002/00000326.jpg\n",
       "2394    video_test_0000002/00000327.jpg\n",
       "2395    video_test_0000002/00000328.jpg\n",
       "2396    video_test_0000002/00000329.jpg\n",
       "2397    video_test_0000002/00000330.jpg\n",
       "Name: filename, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A['valid'] = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "B = [True if i%3==0 else False for i in xrange(len(A)) ]\n",
    "print B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
